"""
Script de ReclasificaciÃ³n Ãšnica - Google Sheets
================================================

Este script descarga TODOS los datos del Google Sheet "puntos flash",
aplica la nueva lÃ³gica de clasificaciÃ³n espacial en 3 pasos, y 
REEMPLAZA completamente el contenido del sheet con los datos reclasificados.

IMPORTANTE:
- Este script debe ejecutarse UNA SOLA VEZ
- Hace BACKUP automÃ¡tico exportando a CSV antes de modificar
- REEMPLAZA todos los datos del sheet

ClasificaciÃ³n en 3 pasos:
1. Palermo Norte â†’ 14.5
2. Anillo Digital C2 â†’ 2.5
3. Comunas â†’ 1.0-15.0
"""

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from google.oauth2 import service_account
import os
import json
import sys
import re
import zipfile
from datetime import datetime

# ConfiguraciÃ³n Google Sheets
NOMBRE_SPREADSHEET = "puntos flash"
NOMBRE_HOJA = "Sheet4"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ConfiguraciÃ³n BigQuery
PROJECT_ID = 'kobo-looker-connect'
DATASET_ID = 'datos_flash'
TABLE_ID = 'kobo_flash_consolidado'
CREDENTIALS_PATH = 'kobo-looker-connect.json'

# Buscar archivos geogrÃ¡ficos
print("ðŸ” Buscando archivos geogrÃ¡ficos...")
RUTA_KMZ_PALERMO = None
RUTA_KML_ANILLO_DIGITAL = None
RUTA_SHP_COMUNAS = None

for root, dirs, files in os.walk(BASE_DIR):
    for file in files:
        if 'palermo' in file.lower() and 'norte' in file.lower() and file.lower().endswith('.kmz'):
            RUTA_KMZ_PALERMO = os.path.join(root, file)
            print(f"   âœ… Palermo Norte: {RUTA_KMZ_PALERMO}")
        
        if 'anillo_digital' in file.lower() and file.lower().endswith('.kmz'):
            RUTA_KML_ANILLO_DIGITAL = os.path.join(root, file)
            print(f"   âœ… Anillo Digital: {RUTA_KML_ANILLO_DIGITAL}")
        
        if file.lower() == 'comunas.shp':
            RUTA_SHP_COMUNAS = os.path.join(root, file)
            print(f"   âœ… Comunas: {RUTA_SHP_COMUNAS}")

if not all([RUTA_KMZ_PALERMO, RUTA_KML_ANILLO_DIGITAL, RUTA_SHP_COMUNAS]):
    print("âŒ ERROR: Faltan archivos geogrÃ¡ficos")
    sys.exit(1)

def clasificar_localizacion_3_pasos(df):
    """
    Aplica clasificaciÃ³n en 3 pasos a un DataFrame con columnas latitude/longitude
    """
    print("\nðŸ—ºï¸ Iniciando clasificaciÃ³n espacial en 3 pasos...")
    
    # Cargar capas geogrÃ¡ficas
    print("ðŸ“‚ Cargando capas...")
    
    # Palermo Norte
    with zipfile.ZipFile(RUTA_KMZ_PALERMO, 'r') as kmz:
        kml_files = [f for f in kmz.namelist() if f.endswith('.kml')]
        if kml_files:
            with kmz.open(kml_files[0]) as kml_file:
                palermo_gdf = gpd.read_file(kml_file)
    if palermo_gdf.crs is None:
        palermo_gdf.set_crs("EPSG:4326", inplace=True)
    palermo_gdf = palermo_gdf.to_crs("EPSG:4326")
    
    # Anillo Digital
    try:
        anillo_digital_gdf = gpd.read_file(RUTA_KML_ANILLO_DIGITAL)
    except:
        if RUTA_KML_ANILLO_DIGITAL.lower().endswith('.kmz'):
            with zipfile.ZipFile(RUTA_KML_ANILLO_DIGITAL, 'r') as kmz:
                kml_files = [f for f in kmz.namelist() if f.endswith('.kml')]
                if kml_files:
                    with kmz.open(kml_files[0]) as kml_file:
                        anillo_digital_gdf = gpd.read_file(kml_file)
        else:
            anillo_digital_gdf = gpd.read_file(RUTA_KML_ANILLO_DIGITAL)
    if anillo_digital_gdf.crs is None:
        anillo_digital_gdf.set_crs("EPSG:4326", inplace=True)
    anillo_digital_gdf = anillo_digital_gdf.to_crs("EPSG:4326")
    
    # Comunas
    comunas_gdf = gpd.read_file(RUTA_SHP_COMUNAS).to_crs("EPSG:4326")
    
    # Convertir puntos a GeoDataFrame
    puntos_gdf = gpd.GeoDataFrame(
        df,
        geometry=gpd.points_from_xy(df.longitude, df.latitude),
        crs="EPSG:4326"
    )
    
    # Inicializar
    puntos_gdf['Localizacion_Nueva'] = None
    
    # PASO 1: Palermo Norte
    print("   ðŸ”¹ Paso 1: Clasificando Palermo Norte...")
    puntos_en_palermo = gpd.sjoin(puntos_gdf, palermo_gdf, how="inner", predicate='within')
    if not puntos_en_palermo.empty:
        puntos_gdf.loc[puntos_en_palermo.index, 'Localizacion_Nueva'] = 14.5
        print(f"      âœ… {len(puntos_en_palermo)} puntos â†’ 14.5 (Palermo Norte)")
    
    # PASO 2: Anillo Digital
    print("   ðŸ”¹ Paso 2: Clasificando Anillo Digital...")
    mask_palermo = puntos_gdf['Localizacion_Nueva'] == 14.5
    puntos_restantes = puntos_gdf[~mask_palermo]
    
    if not puntos_restantes.empty:      
        puntos_en_anillo_digital = gpd.sjoin(puntos_restantes, anillo_digital_gdf, how="inner", predicate='within')
        if not puntos_en_anillo_digital.empty:
            puntos_gdf.loc[puntos_en_anillo_digital.index, 'Localizacion_Nueva'] = 2.5
            print(f"      âœ… {len(puntos_en_anillo_digital)} puntos â†’ 2.5 (Anillo Digital)")
    
    # PASO 3: Comunas
    print("   ðŸ”¹ Paso 3: Clasificando por Comunas...")
    mask_clasificados = puntos_gdf['Localizacion_Nueva'].notna()
    puntos_para_comunas = puntos_gdf[~mask_clasificados]
    
    if not puntos_para_comunas.empty:
        puntos_en_comunas = gpd.sjoin(puntos_para_comunas, comunas_gdf, how="inner", predicate='within')
        if not puntos_en_comunas.empty:
            comuna_col = None
            for col in ['comunas', 'COMUNAS', 'comuna', 'COMUNA', 'NAM', 'ID']:
                if col in puntos_en_comunas.columns:
                    comuna_col = col
                    break
            
            if comuna_col:
                valores_numericos = pd.to_numeric(puntos_en_comunas[comuna_col], errors='coerce')
                puntos_gdf.loc[puntos_en_comunas.index, 'Localizacion_Nueva'] = valores_numericos
                print(f"      âœ… {len(puntos_en_comunas)} puntos â†’ Comunas 1.0-15.0")
    
    # Agregar nueva columna al DataFrame original
    df['Localizacion_Nueva'] = puntos_gdf['Localizacion_Nueva']
    
    return df

def subir_a_bigquery(df):
    """
    Sube el DataFrame a Google BigQuery.
    Trabaja sobre una copia para no afectar los datos de Sheets.
    Limpia nombres de columnas y sanitiza tipos para compatibilidad con BigQuery.
    """
    print("\nðŸ”„ Preparando datos para BigQuery...")
    
    # 1. Clonar DataFrame
    df_bq = df.copy()
    
    # 2. Limpieza de nombres de columnas para BigQuery
    def limpiar_nombre_columna(nombre):
        """Convierte nombres de columnas a formato compatible con BigQuery"""
        if nombre is None:
            return 'unnamed_column'
        nombre = str(nombre)
        nombre = re.sub(r'[ ./()]', '_', nombre)
        nombre = re.sub(r'[^\w]', '_', nombre)
        nombre = re.sub(r'_+', '_', nombre)
        return nombre.strip('_').lower()
    
    df_bq.columns = [limpiar_nombre_columna(col) for col in df_bq.columns]
    print(f"   âœ… Nombres de columnas limpiados para BigQuery")
    
    # 3. SanitizaciÃ³n de tipos complejos (listas/diccionarios)
    for col in df_bq.columns:
        df_bq[col] = df_bq[col].apply(
            lambda x: str(x) if isinstance(x, (list, dict)) else x
        )
    print(f"   âœ… Tipos de datos sanitizados")
    
    # 4. Buscar archivo de credenciales
    possible_names = ['kobo-looker-connect.json', 'credenciales.json', 'service_account.json']
    ruta_creds = None
    
    for name in possible_names:
        for root, _, files in os.walk(BASE_DIR):
            if name in files:
                ruta_creds = os.path.join(root, name)
                break
        if ruta_creds:
            break
    
    if not ruta_creds:
        raise FileNotFoundError(f"No se encontrÃ³ archivo de credenciales para BigQuery")
    
    print(f"   âœ… Usando credenciales: {os.path.basename(ruta_creds)}")
    
    # 5. AutenticaciÃ³n con BigQuery
    credentials = service_account.Credentials.from_service_account_file(
        ruta_creds,
        scopes=["https://www.googleapis.com/auth/bigquery"]
    )
    
    # 6. Carga a BigQuery
    table_full_id = f"{DATASET_ID}.{TABLE_ID}"
    print(f"   ðŸ“¤ Subiendo a BigQuery: {PROJECT_ID}.{table_full_id}")
    
    df_bq.to_gbq(
        destination_table=table_full_id,
        project_id=PROJECT_ID,
        credentials=credentials,
        if_exists='replace',
        progress_bar=False
    )
    
    print(f"   âœ… {len(df_bq)} registros cargados exitosamente a BigQuery")

def main():
    print("="*60)
    print("ðŸ”„ SCRIPT DE RECLASIFICACIÃ“N ÃšNICA - GOOGLE SHEETS")
    print("="*60)
    print("\nâš ï¸ ADVERTENCIA:")
    print("Este script va a:")
    print("1. Descargar TODOS los datos del Google Sheet")
    print("2. Reclasificar usando la nueva lÃ³gica de 3 pasos")
    print("3. REEMPLAZAR completamente el contenido del sheet")
    print("\nðŸ“‹ Nueva clasificaciÃ³n:")
    print("   â€¢ Palermo Norte â†’ 14.5")
    print("   â€¢ Anillo Digital C2 â†’ 2.5")
    print("   â€¢ Comunas â†’ 1.0-15.0")
    
    respuesta = input("\nÂ¿Continuar? (escribe 'SI' para confirmar): ")
    if respuesta.upper() != 'SI':
        print("âŒ OperaciÃ³n cancelada por el usuario")
        sys.exit(0)
    
    # Conectar a Google Sheets
    print("\nðŸ“Š Conectando a Google Sheets...")
    scope = [
        "https://spreadsheets.google.com/feeds",
        'https://www.googleapis.com/auth/spreadsheets',
        "https://www.googleapis.com/auth/drive.file",
        "https://www.googleapis.com/auth/drive"
    ]
    
    if "GOOGLE_CREDENTIALS_JSON" in os.environ:
        creds_dict = json.loads(os.environ["GOOGLE_CREDENTIALS_JSON"])
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    else:
        # Buscar archivo de credenciales con mÃºltiples nombres posibles
        possible_names = ['kobo-looker-connect.json', 'credenciales.json', 'service_account.json']
        ruta_creds = None
        
        for name in possible_names:
            for root, _, files in os.walk(BASE_DIR):
                if name in files:
                    ruta_creds = os.path.join(root, name)
                    print(f"âœ… Credenciales encontradas: {name}")
                    break
            if ruta_creds:
                break
        
        if not ruta_creds:
            print("âŒ ERROR: No se encontrÃ³ archivo de credenciales")
            print(f"   Buscando: {', '.join(possible_names)}")
            sys.exit(1)
        
        creds = ServiceAccountCredentials.from_json_keyfile_name(ruta_creds, scope)
    
    client = gspread.authorize(creds)
    sheet = client.open(NOMBRE_SPREADSHEET).worksheet(NOMBRE_HOJA)
    
    # Descargar todos los datos
    print("â¬‡ï¸ Descargando datos del sheet...")
    registros = sheet.get_all_records()
    
    if not registros:
        print("âŒ El sheet estÃ¡ vacÃ­o, no hay nada que reclasificar")
        sys.exit(1)
    
    df = pd.DataFrame(registros)
    print(f"   âœ… Descargados {len(df)} registros")
    
    # Hacer backup
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = f"backup_sheet_{timestamp}.csv"
    df.to_csv(backup_file, index=False)
    print(f"ðŸ’¾ Backup guardado: {backup_file}")
    
    # Extraer coordenadas si estÃ¡n en columna geo_ref
    if 'geo_ref/geo_punto' in df.columns:
        split_coords = df['geo_ref/geo_punto'].astype(str).str.split(' ', expand=True)
        if split_coords.shape[1] >= 2:
            df['latitude'] = pd.to_numeric(split_coords[0], errors='coerce')
            df['longitude'] = pd.to_numeric(split_coords[1], errors='coerce')
    
    if 'latitude' not in df.columns or 'longitude' not in df.columns:
        print("âŒ ERROR: No se encontraron columnas latitude/longitude")
        sys.exit(1)
    
    # Eliminar filas sin coordenadas vÃ¡lidas
    df_con_coords = df.dropna(subset=['latitude', 'longitude']).copy()
    print(f"ðŸ“ Registros con coordenadas vÃ¡lidas: {len(df_con_coords)}")
    
    # Aplicar reclasificaciÃ³n
    df_reclasificado = clasificar_localizacion_3_pasos(df_con_coords)
    
    # Mostrar estadÃ­sticas
    print("\nðŸ“Š Resultados de reclasificaciÃ³n:")
    print(f"   â€¢ Palermo Norte (14.5): {(df_reclasificado['Localizacion_Nueva'] == 14.5).sum()}")
    print(f"   â€¢ Anillo Digital C2 (2.5): {(df_reclasificado['Localizacion_Nueva'] == 2.5).sum()}")
    print(f"   â€¢ Comuna 1-15: {((df_reclasificado['Localizacion_Nueva'] >= 1) & (df_reclasificado['Localizacion_Nueva'] <= 15)).sum()}")
    print(f"   â€¢ Sin clasificar: {df_reclasificado['Localizacion_Nueva'].isna().sum()}")
    
    # Reemplazar columna Localizacion con Localizacion_Nueva
    if 'Localizacion' in df_reclasificado.columns:
        df_reclasificado['Localizacion'] = df_reclasificado['Localizacion_Nueva']
        df_reclasificado = df_reclasificado.drop(columns=['Localizacion_Nueva'])
    else:
        df_reclasificado = df_reclasificado.rename(columns={'Localizacion_Nueva': 'Localizacion'})
    
    # Convertir a object y reemplazar NaN con None para Google Sheets
    df_final = df_reclasificado.astype(object)
    df_final = df_final.where(pd.notnull(df_final), None)
    
    # Subir a Google Sheets
    print("\nâ¬†ï¸ Subiendo datos reclasificados al sheet...")
    respuesta_final = input("Â¿Confirmas el reemplazo del sheet? (escribe 'SI'): ")
    if respuesta_final.upper() != 'SI':
        print("âŒ OperaciÃ³n cancelada")
        sys.exit(0)
    
    sheet.clear()
    sheet.update(
        values=[df_final.columns.values.tolist()] + df_final.values.tolist(),
        value_input_option='USER_ENTERED'
    )
    
    # Subir a BigQuery
    print("\nðŸ“¤ Subiendo a BigQuery...")
    try:
        subir_a_bigquery(df_final)
        print("   âœ… Carga a BigQuery exitosa")
    except Exception as e:
        print(f"   âš ï¸  Error en BigQuery (no crÃ­tico): {e}")
        print(f"   â„¹ï¸  Los datos en Google Sheets se actualizaron correctamente")
    
    print("\n" + "="*60)
    print("âœ… RECLASIFICACIÃ“N COMPLETADA EXITOSAMENTE")
    print("="*60)
    print(f"ðŸ“ Backup guardado en: {backup_file}")
    print(f"ðŸ“Š Total registros procesados: {len(df_final)}")
    print(f"ðŸ“Š Tabla BigQuery: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}")
    print("\nðŸŽ‰ El Google Sheet y BigQuery han sido actualizados con la nueva clasificaciÃ³n")

if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ ERROR FATAL: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
